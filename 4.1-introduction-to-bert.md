---
hidden: true
---

# ðŸŸ¡ 4.1 Introduction to BERT

**BERT:**

* Bi-directional Encoder Representation from Transformers
* Auto encoding language model
* Uses only encoder from the Transformer
* Relying on self attention
* The encoder is taken from the transformer architecture

***

* Consider the following sentence: â€˜I love my pet pythonâ€™
* We feed this sentence into BERT to get a context-ful representation(vector embedding) of every word in the sentence
* The encoder understands the context of each word in the sentence using a multi-head attention mechanism (which relates each word to every other word in the sentence)
*

    <figure><img src=".gitbook/assets/image (102).png" alt=""><figcaption></figcaption></figure>
* Here we feed it a sequence of tokens through the encoder stack
* The output of BERT is a representation of every single token in the sequence
* \[CLS] â€“ Special reserved token, specially for BERT, put at the beginning of every single sequence, and its goal is to represent the entire input
* This token will be used for doing tasks such as sequence classification
* \[SEP] is put at the end, which stands for separator is meant to represent a separation between 2 sequences
* Because BERT is able to take in either one or two sequences at a time depending on the task
* We pass only 1 sequences for classification
* We pass 2 sequences for task such as question and answering, where sequences 1 is question and sequence 2 is answer
* Each box at the top represents the vector representation of every token

***

**BERT size:**

*

    <figure><img src=".gitbook/assets/image (103).png" alt=""><figcaption></figcaption></figure>
* BERT base has 12 encoder
* BERT small has 4 encoder
* BERT large has 24 encoder

***

*

    <figure><img src=".gitbook/assets/image (104).png" alt=""><figcaption></figcaption></figure>
* Before the input reaches the Encoder, we need to tokenize it
* A token sometimes does not represent an entire word and one word may not be represented by a single token
* Each encoder is a combination of multi headed attention and add norm feedforward network, this promotes generalizability and prevent overfitting and also speed up training

```python
model = BertModel.from_pretrained('bert-base-uncased')

-Â Â Â Â Â Â Â Â Â Â Â  The BERT model has 199 different named parameters.
-Â Â Â Â Â Â Â Â Â Â Â  Â 
-Â Â Â Â Â Â Â Â Â Â Â  ==== Embedding Layer ====
-Â Â Â Â Â Â Â Â Â Â Â  Â 
-Â Â Â Â Â Â Â Â Â Â Â  embeddings.word_embeddings.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (30522, 768)
-Â Â Â Â Â Â Â Â Â Â Â  embeddings.position_embeddings.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (512, 768)
-Â Â Â Â Â Â Â Â Â Â Â  embeddings.token_type_embeddings.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (2, 768)
-Â Â Â Â Â Â Â Â Â Â Â  embeddings.LayerNorm.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  embeddings.LayerNorm.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  Â 
-Â Â Â Â Â Â Â Â Â Â Â  ==== First Encoder ====
-Â Â Â Â Â Â Â Â Â Â Â  Â 
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.self.query.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â  (768, 768)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.self.query.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.self.key.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768, 768)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.self.key.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.self.value.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â  (768, 768)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.self.value.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.output.dense.weightÂ Â Â Â Â Â Â Â Â Â Â Â  (768, 768)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.output.dense.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.output.LayerNorm.weightÂ Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.attention.output.LayerNorm.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.intermediate.dense.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (3072, 768)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.intermediate.dense.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (3072,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.output.dense.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768, 3072)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.output.dense.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.output.LayerNorm.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  encoder.layer.0.output.LayerNorm.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-Â Â Â Â Â Â Â Â Â Â Â  Â 
-Â Â Â Â Â Â Â Â Â Â Â  ==== Output Layer ====
-Â Â Â Â Â Â Â Â Â Â Â  Â 
-Â Â Â Â Â Â Â Â Â Â Â  pooler.dense.weightÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768, 768)
-Â Â Â Â Â Â Â Â Â Â Â  pooler.dense.biasÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (768,)
-             

#Embedding layer:
# BERT is aware of 30522 tokens
# Each of these tokens has a contextless embedding of 768
# Output layer has a pooler
# Pooler is a separate feed-forward network with a hyperbolic tangent 
# activation function that acts on [CLS] token representation
# When we are using BERT, the pooler takes the embedding of [CLS] token, 
# the one that is supposed to represent the entire sequence and it is adding a 
# layer on top of it, so that we can use the output of the pooler as a vector 
# representation of the entire sequence
# The pooler acts a representation for the entire input sequence
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer.encode('Sinan loves a beautiful day')  # tokenize a simple sequence
#Output:
# [101, 8254, 2319, 7459, 1037, 3376, 2154, 102]
# 101 is for [CLS] and 102 is for [SEP]
 
# run tokens through the model
#1 Turn tokens_with_unknown_words into a tensor (will be size (8,))
#2 Unsqueeze a first dimension to simulate batches. Resulting shape is (1, 8)
response = model(torch.tensor(tokenizer.encode('Sinan loves a beautiful day')).unsqueeze(0))
# When we pass it through BERT, we get many outputs
# Here the last hidden state, is the output of the 12th encoder
# Here the pooler output is to be used as a vector that represents the entire 
# sequence as a whole
```

\- &#x20;
