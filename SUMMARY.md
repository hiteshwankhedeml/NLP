# Table of contents

* [🟢 Roadmap](README.md)
* [🟢 Practical use cases of NLP](practical-use-cases-of-nlp.md)
* [🟢 Approaches to NLP](approaches-to-nlp.md)
* [🟢 End to End NLP Pipeline](end-to-end-nlp-pipeline.md)
* [🟢 Syntax, Semantic and Pragmatics](syntax-semantic-and-pragmatics.md)
* [✈️ Pre-processing steps](pre-processing-steps.md)
* [🔴 Text cleaning](text-cleaning.md)
* [🟢 Tokenization and Basic Techniques](tokenization-and-basic-techniques.md)
* [🟢 Tokenization Practicals](tokenization-practicals.md)
* [🟢 Stemming using NLTK](stemming-using-nltk.md)
* [✈️ Stemming](stemming.md)
* [🟢 Lemmatization using NLTK](lemmatization-using-nltk.md)
* [✈️ Lemmatization](lemmatization.md)
* [🏁 ---------- 10 ----------](10.md)
* [🟢 Stopwords](stopwords.md)
* [🟢 POS using NLTK](pos-using-nltk.md)
* [🟢 Named Entity Recognition](named-entity-recognition.md)
* [✈️ What's Next](whats-next.md)
* [🟢 Encoding vs Embedding](encoding-vs-embedding.md)
* [✈️ Encoding and Embedding](encoding-and-embedding.md)
* [✈️ Vector](vector.md)
* [🟢 One Hot Encoding](one-hot-encoding.md)
* [✈️ One Hot Encoding - 2](one-hot-encoding-2.md)
* [🟢 OHE Advantages and Disadvantages](ohe-advantages-and-disadvantages.md)
* [🟢 Bag of words intuition](bag-of-words-intuition.md)
* [🟢 BOW Advantages and Disadvantages](bow-advantages-and-disadvantages.md)
* [🟢 BOW implementation using NLTK](bow-implementation-using-nltk.md)
* [🟢 N Grams](n-grams.md)
* [----------  20 ----------](20.md)
* [✈️ N Grams - 2](n-grams-2.md)
* [🟢 N Gram BOW implementation using NLTK](n-gram-bow-implementation-using-nltk.md)
* [TF IDF Intuition](tf-idf-intuition.md)
* [🟢 TF-IDF](tf-idf.md)
* [🟢 TF IDF Advantages and Disadvantages](tf-idf-advantages-and-disadvantages.md)
* [🟢 TF IDF implementation](tf-idf-implementation.md)
* [🟢 BM25](bm25.md)
* [🟢 BM25 - Calculations](bm25-calculations.md)
* [🟢 Word Embeddings](word-embeddings.md)
* [🟢 Word2Vec intuition](word2vec-intuition.md)
* [🟢 Word2Vec Cbow Intuition](word2vec-cbow-intuition.md)
* [🟢 Skipgram Indepth Intuition](skipgram-indepth-intuition.md)
* [---------- 30  ----------](30.md)
* [🟢 Advantages of Word2Vec](advantages-of-word2vec.md)
* [🟢 Average Word2Vec intuition](average-word2vec-intuition.md)
* [🟢 Word2Vec - Training](word2vec-training.md)
* [🟢 Custom training for word2vec](custom-training-for-word2vec.md)
* [🟢 Word2Vec practical implementation - Gensim](word2vec-practical-implementation-gensim.md)
* [🟢 Spam Ham project using BOW](spam-ham-project-using-bow.md)
* [🟢 Spam Ham using Tf-Idf](spam-ham-using-tf-idf.md)
* [🟢 Best practices for solving ML problems](best-practices-for-solving-ml-problems.md)
* [✈️ Word2Vec - For your own data](word2vec-for-your-own-data.md)
* [🟢 Text Classification with Word2Vec and AvgWord2Vec](text-classification-with-word2vec-and-avgword2vec.md)
* [🟢 Kindle review sentiment analysis](kindle-review-sentiment-analysis.md)
* [---------- 40 ----------](40.md)
* [🟢 Sentence embedding using sentence transformer](sentence-embedding-using-sentence-transformer.md)
* [Sentence embedding using Huggingface](sentence-embedding-using-huggingface.md)
* [Sentence embedding using transformers](sentence-embedding-using-transformers.md)
* [Label Encoder](label-encoder.md)
* [Letter Embedding](letter-embedding.md)
* [Letter Embedding- Code](letter-embedding-code.md)
* [Code - Text Generation](code-text-generation.md)
* [Introduction to NLP in DL](introduction-to-nlp-in-dl.md)
* [ANN vs RNN](ann-vs-rnn.md)
* [RNN Overview](rnn-overview.md)
* [---------- 50 ----------](50.md)
* [Sequential Memory](sequential-memory.md)
* [Short term memory problem](short-term-memory-problem.md)
* [Data passing to RNN](data-passing-to-rnn.md)
* [Types of Configuration](types-of-configuration.md)
* [---------- 60 ----------](60.md)
* [RNN forward propagation with time](rnn-forward-propagation-with-time.md)
* [Trainable Parameters](trainable-parameters.md)
* [RNN forward propagation with time](rnn-forward-propagation-with-time-1.md)
* [Simple RNN Backward Propagation](simple-rnn-backward-propagation.md)
* [Simple RNN Backward Propagation - ChatGPT Calculations](simple-rnn-backward-propagation-chatgpt-calculations.md)
* [Problems with RNN](problems-with-rnn.md)
* [Equations](equations.md)
* [Code - Classifier](code-classifier.md)
* [Evaluation Metrics, Loss Function](evaluation-metrics-loss-function.md)
* [ℹ️ End to End Deep Learning Project with RNN](end-to-end-deep-learning-project-with-rnn.md)
* [Problem Statement](problem-statement.md)
* [---------- 70 ----------](70.md)
* [Getting started with embedding layers](getting-started-with-embedding-layers.md)
* [Implementing Word Embedding with Keras Tensorflow](implementing-word-embedding-with-keras-tensorflow.md)
* [Loading and Understanding Dataset and Feature Engineering](loading-and-understanding-dataset-and-feature-engineering.md)
* [Training simple RNN with embedding layers](training-simple-rnn-with-embedding-layers.md)
* [Prediction from trained Simple RNN](prediction-from-trained-simple-rnn.md)
* [End to End Streamlit Web App Integrated with RNN and Deployment](end-to-end-streamlit-web-app-integrated-with-rnn-and-deployment.md)
* [Simple RNN with Pretrained embeddings](simple-rnn-with-pretrained-embeddings.md)
* [Why LSTM RNN](why-lstm-rnn.md)
* [Basic Representation of RNN and LSTM RNN](basic-representation-of-rnn-and-lstm-rnn.md)
* [LSTM RNN Architecture](lstm-rnn-architecture.md)
* [---------- 80 ----------](80.md)
* [Forget Gate](forget-gate.md)
* [Input Gate and Candidate memory](input-gate-and-candidate-memory.md)
* [Combination of Forget and Input](combination-of-forget-and-input.md)
* [Output Gate](output-gate.md)
* [Variants of LSTM RNN - Peephole](variants-of-lstm-rnn-peephole.md)
* [Behavior of Network over time](behavior-of-network-over-time.md)
* [Variant of LSTM](variant-of-lstm.md)
* [Stacking](stacking.md)
* [ℹ️ LSTM and GRU End to End Project - Next Word Prediction](lstm-and-gru-end-to-end-project-next-word-prediction.md)
* [Data Collection and Pre Processing](data-collection-and-pre-processing.md)
* [Training](training.md)
* [---------- 90 ----------](90.md)
* [Prediction](prediction.md)
* [Streamlit WebApp](streamlit-webapp.md)
* [GRU Main Simplifications](gru-main-simplifications.md)
* [GRU RNN](gru-rnn.md)
* [GRU Equations](gru-equations.md)
* [Types of RNN](types-of-rnn.md)
* [Bi RNN - Overview](bi-rnn-overview.md)
* [Bidirectional RNN](bidirectional-rnn.md)
* [Code - Sentiment Analysis](code-sentiment-analysis.md)
* [ℹ️ Encoder Decoder](encoder-decoder.md)
* [Neural Machine Translation](neural-machine-translation.md)
* [---------- 100 ----------](100.md)
* [Sequence Length](sequence-length.md)
* [Softmax](softmax.md)
* [Prediction](prediction-1.md)
* [Embedding Layer](embedding-layer.md)
* [Intuiton](intuiton.md)
* [Why LSTM/GRU cannot be used for Language Translation](why-lstm-gru-cannot-be-used-for-language-translation.md)
* [Problems](problems.md)
* [Attention - Introduction](attention-introduction.md)
* [Key Points](key-points.md)
* [Self Attention](self-attention.md)
* [---------- 110 ----------](110.md)
* [Attention is all you need](attention-is-all-you-need.md)
* [Attention Mechanism - Seq2Seq](attention-mechanism-seq2seq.md)
* [Attention Mechanism - Blog](attention-mechanism-blog.md)
* [🟢 What and Why to use Transformers](what-and-why-to-use-transformers.md)
* [🟢 Understanding the basic architecture of Encoder](understanding-the-basic-architecture-of-encoder.md)
* [🟢 Self attention layer working](self-attention-layer-working.md)
* [🟢 Multi head attention](multi-head-attention.md)
* [🟢 Feed Forward NN with Multi Head Attention](feed-forward-nn-with-multi-head-attention.md)
* [🟢 Positional Encoding](positional-encoding.md)
* [Layer Normalization](layer-normalization.md)
* [Layer Normalization Example](layer-normalization-example.md)
* [---------- 120 ----------](120.md)
* [Dataloader and Dataset](dataloader-and-dataset.md)
* [Transformer Code](transformer-code.md)
* [Research Paper - Transformer](research-paper-transformer.md)
* [Research Paper - Transformer and ULMFit](research-paper-transformer-and-ulmfit.md)
* [🔴 Sentence Transformer](sentence-transformer.md)
* [INTRODUCTION TO TRANSFORMERS FOR NLP](introduction-to-transformers-for-nlp.md)
* [1.1 A Brief History of NLP](1.1-a-brief-history-of-nlp.md)
* [1.2 Pay Attention with Attention](1.2-pay-attention-with-attention.md)
* [1.3 Encoder and Decoder Architecture](1.3-encoder-and-decoder-architecture.md)
* [1.4 How Language Model Looks at text](1.4-how-language-model-looks-at-text.md)
* [2.1 Introduction to Transformers](2.1-introduction-to-transformers.md)
* [---------- 130 ----------](130.md)
* [2.2 Scaled Dot Product Attention](2.2-scaled-dot-product-attention.md)
* [2.3 Multi Headed Attention](2.3-multi-headed-attention.md)
* [3.1 Introduction to Transfer Learning](3.1-introduction-to-transfer-learning.md)
* [3.2 Introduction to PyTorch](3.2-introduction-to-pytorch.md)
* [3.3 Fine tuning Transformers with PyTorch](3.3-fine-tuning-transformers-with-pytorch.md)
* [4.1 Introduction to BERT](4.1-introduction-to-bert.md)
* [4.2 Wordpiece Tokenization](4.2-wordpiece-tokenization.md)
* [4.3 The many embeddings of BERT](4.3-the-many-embeddings-of-bert.md)
* [5.1 The Masked Language Modelling Task](5.1-the-masked-language-modelling-task.md)
* [5.2 Next Sentence Prediction Task](5.2-next-sentence-prediction-task.md)
* [---------- 140 ----------](140.md)
* [5.3 Fine Tuning BERT to solve NLP Problems](5.3-fine-tuning-bert-to-solve-nlp-problems.md)
* [6.1 Flavors of BERT](6.1-flavors-of-bert.md)
* [6.2 BERT for Sequence Classification](6.2-bert-for-sequence-classification.md)
* [6.3 BERT for Token Classification](6.3-bert-for-token-classification.md)
* [6.4 BERT for QA](6.4-bert-for-qa.md)
