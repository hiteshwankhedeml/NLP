# Table of contents

* [ğŸŸ¢ Roadmap](README.md)
* [ğŸŸ¢ Practical use cases of NLP](practical-use-cases-of-nlp.md)
* [ğŸŸ¢ Approaches to NLP](approaches-to-nlp.md)
* [ğŸŸ¢ End to End NLP Pipeline](end-to-end-nlp-pipeline.md)
* [ğŸŸ¢ Syntax, Semantic and Pragmatics](syntax-semantic-and-pragmatics.md)
* [âœˆï¸ Pre-processing steps](pre-processing-steps.md)
* [ğŸ”´ Text cleaning](text-cleaning.md)
* [ğŸŸ¢ Tokenization and Basic Techniques](tokenization-and-basic-techniques.md)
* [ğŸŸ¢ Tokenization Practicals](tokenization-practicals.md)
* [ğŸŸ¢ Stemming using NLTK](stemming-using-nltk.md)
* [âœˆï¸ Stemming](stemming.md)
* [ğŸŸ¢ Lemmatization using NLTK](lemmatization-using-nltk.md)
* [âœˆï¸ Lemmatization](lemmatization.md)
* [ğŸ ---------- 10 ----------](10.md)
* [ğŸŸ¢ Stopwords](stopwords.md)
* [ğŸŸ¢ POS using NLTK](pos-using-nltk.md)
* [ğŸŸ¢ Named Entity Recognition](named-entity-recognition.md)
* [âœˆï¸ What's Next](whats-next.md)
* [Encoding vs Embedding](encoding-vs-embedding.md)
* [Encoding and Embedding](encoding-and-embedding.md)
* [Vector](vector.md)
* [One Hot Encoding](one-hot-encoding.md)
* [One Hot Encoding - 2](one-hot-encoding-2.md)
* [OHE Advantages and Disadvantages](ohe-advantages-and-disadvantages.md)
* [Bag of words intuition](bag-of-words-intuition.md)
* [BOW Advantages and Disadvantages](bow-advantages-and-disadvantages.md)
* [BOW implementation using NLTK](bow-implementation-using-nltk.md)
* [N Grams](n-grams.md)
* [N Grams - 2](n-grams-2.md)
* [N Gram BOW implementation using NLTK](n-gram-bow-implementation-using-nltk.md)
* [TF IDF Intuition](tf-idf-intuition.md)
* [TF IDF Advantages and Disadvantages](tf-idf-advantages-and-disadvantages.md)
* [TF IDF implementation](tf-idf-implementation.md)
* [BM25](bm25.md)
* [Sparse Matrix vs Dense Matrix](sparse-matrix-vs-dense-matrix.md)
* [Word Embeddings](word-embeddings.md)
* [Word2Vec intuition](word2vec-intuition.md)
* [Word2Vec Cbow Intuition](word2vec-cbow-intuition.md)
* [Skipgram Indepth Intuition](skipgram-indepth-intuition.md)
* [Advantages of Word2Vec](advantages-of-word2vec.md)
* [Average Word2Vec intuition](average-word2vec-intuition.md)
* [Word2Vec - Training](word2vec-training.md)
* [Custom training for word2vec](custom-training-for-word2vec.md)
* [Word2Vec practical implementation - Gensim](word2vec-practical-implementation-gensim.md)
* [Spam Ham project using BOW](spam-ham-project-using-bow.md)
* [Spam Ham using Tf-Idf](spam-ham-using-tf-idf.md)
* [Best practices for solving ML problems](best-practices-for-solving-ml-problems.md)
* [Word2Vec - For your own data](word2vec-for-your-own-data.md)
* [Text Classification with Word2Vec and AvgWord2Vec](text-classification-with-word2vec-and-avgword2vec.md)
* [Kindle review sentiment analysis](kindle-review-sentiment-analysis.md)
* [Sentence embedding using sentence transformer](sentence-embedding-using-sentence-transformer.md)
* [Sentence embedding using Huggingface](sentence-embedding-using-huggingface.md)
* [Sentence embedding using transformers](sentence-embedding-using-transformers.md)
* [Label Encoder](label-encoder.md)
* [Letter Embedding](letter-embedding.md)
* [Letter Embedding- Code](letter-embedding-code.md)
* [Code - Text Generation](code-text-generation.md)
* [Introduction to NLP in DL](introduction-to-nlp-in-dl.md)
* [ANN vs RNN](ann-vs-rnn.md)
* [RNN Overview](rnn-overview.md)
* [Sequential Memory](sequential-memory.md)
* [Short term memory problem](short-term-memory-problem.md)
* [Data passing to RNN](data-passing-to-rnn.md)
* [Types of Configuration](types-of-configuration.md)
* [RNN forward propagation with time](rnn-forward-propagation-with-time.md)
* [Trainable Parameters](trainable-parameters.md)
* [RNN forward propagation with time](rnn-forward-propagation-with-time-1.md)
* [Simple RNN Backward Propagation](simple-rnn-backward-propagation.md)
* [Simple RNN Backward Propagation - ChatGPT Calculations](simple-rnn-backward-propagation-chatgpt-calculations.md)
* [Problems with RNN](problems-with-rnn.md)
* [Equations](equations.md)
* [Code - Classifier](code-classifier.md)
* [Evaluation Metrics, Loss Function](evaluation-metrics-loss-function.md)
* [â„¹ï¸ End to End Deep Learning Project with RNN](end-to-end-deep-learning-project-with-rnn.md)
* [Problem Statement](problem-statement.md)
* [Getting started with embedding layers](getting-started-with-embedding-layers.md)
* [Implementing Word Embedding with Keras Tensorflow](implementing-word-embedding-with-keras-tensorflow.md)
* [Loading and Understanding Dataset and Feature Engineering](loading-and-understanding-dataset-and-feature-engineering.md)
* [Training simple RNN with embedding layers](training-simple-rnn-with-embedding-layers.md)
* [Prediction from trained Simple RNN](prediction-from-trained-simple-rnn.md)
* [End to End Streamlit Web App Integrated with RNN and Deployment](end-to-end-streamlit-web-app-integrated-with-rnn-and-deployment.md)
* [Simple RNN with Pretrained embeddings](simple-rnn-with-pretrained-embeddings.md)
* [Why LSTM RNN](why-lstm-rnn.md)
* [Basic Representation of RNN and LSTM RNN](basic-representation-of-rnn-and-lstm-rnn.md)
* [LSTM RNN Architecture](lstm-rnn-architecture.md)
* [Forget Gate](forget-gate.md)
* [Input Gate and Candidate memory](input-gate-and-candidate-memory.md)
* [Combination of Forget and Input](combination-of-forget-and-input.md)
* [Output Gate](output-gate.md)
* [Variants of LSTM RNN - Peephole](variants-of-lstm-rnn-peephole.md)
