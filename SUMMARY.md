# Table of contents

* [üü¢ Roadmap](README.md)
* [üü¢ Practical use cases of NLP](practical-use-cases-of-nlp.md)
* [üü¢ Approaches to NLP](approaches-to-nlp.md)
* [üü¢ End to End NLP Pipeline](end-to-end-nlp-pipeline.md)
* [üü¢ Syntax, Semantic and Pragmatics](syntax-semantic-and-pragmatics.md)
* [‚úàÔ∏è Pre-processing steps](pre-processing-steps.md)
* [üî¥ Text cleaning](text-cleaning.md)
* [üü¢ Tokenization and Basic Techniques](tokenization-and-basic-techniques.md)
* [üü¢ Tokenization Practicals](tokenization-practicals.md)
* [üü¢ Stemming using NLTK](stemming-using-nltk.md)
* [‚úàÔ∏è Stemming](stemming.md)
* [üü¢ Lemmatization using NLTK](lemmatization-using-nltk.md)
* [‚úàÔ∏è Lemmatization](lemmatization.md)
* [üèÅ ---------- 10 ----------](10.md)
* [üü¢ Stopwords](stopwords.md)
* [üü¢ POS using NLTK](pos-using-nltk.md)
* [üü¢ Named Entity Recognition](named-entity-recognition.md)
* [‚úàÔ∏è What's Next](whats-next.md)
* [Encoding vs Embedding](encoding-vs-embedding.md)
* [Encoding and Embedding](encoding-and-embedding.md)
* [Vector](vector.md)
* [One Hot Encoding](one-hot-encoding.md)
* [One Hot Encoding - 2](one-hot-encoding-2.md)
* [OHE Advantages and Disadvantages](ohe-advantages-and-disadvantages.md)
* [Bag of words intuition](bag-of-words-intuition.md)
* [BOW Advantages and Disadvantages](bow-advantages-and-disadvantages.md)
* [BOW implementation using NLTK](bow-implementation-using-nltk.md)
* [N Grams](n-grams.md)
* [N Grams - 2](n-grams-2.md)
* [N Gram BOW implementation using NLTK](n-gram-bow-implementation-using-nltk.md)
* [TF IDF Intuition](tf-idf-intuition.md)
* [TF IDF Advantages and Disadvantages](tf-idf-advantages-and-disadvantages.md)
* [TF IDF implementation](tf-idf-implementation.md)
* [BM25](bm25.md)
* [Sparse Matrix vs Dense Matrix](sparse-matrix-vs-dense-matrix.md)
* [Word Embeddings](word-embeddings.md)
* [Word2Vec intuition](word2vec-intuition.md)
* [Word2Vec Cbow Intuition](word2vec-cbow-intuition.md)
* [Skipgram Indepth Intuition](skipgram-indepth-intuition.md)
* [Advantages of Word2Vec](advantages-of-word2vec.md)
* [Average Word2Vec intuition](average-word2vec-intuition.md)
* [Word2Vec - Training](word2vec-training.md)
* [Custom training for word2vec](custom-training-for-word2vec.md)
* [Word2Vec practical implementation - Gensim](word2vec-practical-implementation-gensim.md)
* [Spam Ham project using BOW](spam-ham-project-using-bow.md)
* [Spam Ham using Tf-Idf](spam-ham-using-tf-idf.md)
* [Best practices for solving ML problems](best-practices-for-solving-ml-problems.md)
* [Word2Vec - For your own data](word2vec-for-your-own-data.md)
* [Text Classification with Word2Vec and AvgWord2Vec](text-classification-with-word2vec-and-avgword2vec.md)
* [Kindle review sentiment analysis](kindle-review-sentiment-analysis.md)
* [Sentence embedding using sentence transformer](sentence-embedding-using-sentence-transformer.md)
* [Sentence embedding using Huggingface](sentence-embedding-using-huggingface.md)
* [Sentence embedding using transformers](sentence-embedding-using-transformers.md)
* [Label Encoder](label-encoder.md)
* [Letter Embedding](letter-embedding.md)
* [Letter Embedding- Code](letter-embedding-code.md)
* [Code - Text Generation](code-text-generation.md)
* [Introduction to NLP in DL](introduction-to-nlp-in-dl.md)
* [ANN vs RNN](ann-vs-rnn.md)
* [RNN Overview](rnn-overview.md)
* [Sequential Memory](sequential-memory.md)
* [Short term memory problem](short-term-memory-problem.md)
* [Data passing to RNN](data-passing-to-rnn.md)
* [Types of Configuration](types-of-configuration.md)
* [RNN forward propagation with time](rnn-forward-propagation-with-time.md)
* [Trainable Parameters](trainable-parameters.md)
* [RNN forward propagation with time](rnn-forward-propagation-with-time-1.md)
* [Simple RNN Backward Propagation](simple-rnn-backward-propagation.md)
* [Simple RNN Backward Propagation - ChatGPT Calculations](simple-rnn-backward-propagation-chatgpt-calculations.md)
* [Problems with RNN](problems-with-rnn.md)
* [Equations](equations.md)
* [Code - Classifier](code-classifier.md)
* [Evaluation Metrics, Loss Function](evaluation-metrics-loss-function.md)
* [‚ÑπÔ∏è End to End Deep Learning Project with RNN](end-to-end-deep-learning-project-with-rnn.md)
* [Problem Statement](problem-statement.md)
* [Getting started with embedding layers](getting-started-with-embedding-layers.md)
* [Implementing Word Embedding with Keras Tensorflow](implementing-word-embedding-with-keras-tensorflow.md)
* [Loading and Understanding Dataset and Feature Engineering](loading-and-understanding-dataset-and-feature-engineering.md)
* [Training simple RNN with embedding layers](training-simple-rnn-with-embedding-layers.md)
* [Prediction from trained Simple RNN](prediction-from-trained-simple-rnn.md)
* [End to End Streamlit Web App Integrated with RNN and Deployment](end-to-end-streamlit-web-app-integrated-with-rnn-and-deployment.md)
* [Simple RNN with Pretrained embeddings](simple-rnn-with-pretrained-embeddings.md)
* [Why LSTM RNN](why-lstm-rnn.md)
* [Basic Representation of RNN and LSTM RNN](basic-representation-of-rnn-and-lstm-rnn.md)
* [LSTM RNN Architecture](lstm-rnn-architecture.md)
* [Forget Gate](forget-gate.md)
* [Input Gate and Candidate memory](input-gate-and-candidate-memory.md)
* [Combination of Forget and Input](combination-of-forget-and-input.md)
* [Output Gate](output-gate.md)
* [Variants of LSTM RNN - Peephole](variants-of-lstm-rnn-peephole.md)
* [Behavior of Network over time](behavior-of-network-over-time.md)
* [Variant of LSTM](variant-of-lstm.md)
* [Stacking](stacking.md)
* [‚ÑπÔ∏è LSTM and GRU End to End Project - Next Word Prediction](lstm-and-gru-end-to-end-project-next-word-prediction.md)
* [Data Collection and Pre Processing](data-collection-and-pre-processing.md)
* [Training](training.md)
* [Prediction](prediction.md)
* [Streamlit WebApp](streamlit-webapp.md)
* [GRU Main Simplifications](gru-main-simplifications.md)
* [GRU RNN](gru-rnn.md)
* [GRU Equations](gru-equations.md)
* [Types of RNN](types-of-rnn.md)
* [Bi RNN - Overview](bi-rnn-overview.md)
* [Bidirectional RNN](bidirectional-rnn.md)
* [Code - Sentiment Analysis](code-sentiment-analysis.md)
* [‚ÑπÔ∏è Encoder Decoder](encoder-decoder.md)
* [Neural Machine Translation](neural-machine-translation.md)
* [Sequence Length](sequence-length.md)
* [Softmax](softmax.md)
* [Prediction](prediction-1.md)
* [Embedding Layer](embedding-layer.md)
* [Intuiton](intuiton.md)
* [Why LSTM/GRU cannot be used for Language Translation](why-lstm-gru-cannot-be-used-for-language-translation.md)
* [Problems](problems.md)
* [Attention - Introduction](attention-introduction.md)
* [Key Points](key-points.md)
* [Self Attention](self-attention.md)
* [Attention is all you need](attention-is-all-you-need.md)
* [Attention Mechanism - Seq2Seq](attention-mechanism-seq2seq.md)
* [Attention Mechanism - Blog](attention-mechanism-blog.md)
* [What and Why to use Transformers](what-and-why-to-use-transformers.md)
* [Understanding the basic architecture of Encoder](understanding-the-basic-architecture-of-encoder.md)
* [Self attention layer working](self-attention-layer-working.md)
* [Multi head attention](multi-head-attention.md)
* [Feed Forward NN with Multi Head Attention](feed-forward-nn-with-multi-head-attention.md)
* [Positional Encoding](positional-encoding.md)
* [Layer Normalization](layer-normalization.md)
* [Dataloader and Dataset](dataloader-and-dataset.md)
* [Transformer Code](transformer-code.md)
* [Research Paper - Transformer](research-paper-transformer.md)
* [Research Paper - Transformer and ULMFit](research-paper-transformer-and-ulmfit.md)
* [Page 19](page-19.md)
* [Page 18](page-18.md)
* [Page 17](page-17.md)
* [Page 16](page-16.md)
* [Page 15](page-15.md)
* [Page 14](page-14.md)
* [Page 13](page-13.md)
* [Page 12](page-12.md)
* [Page 11](page-11.md)
* [Page 10](page-10.md)
* [Page 9](page-9.md)
* [Page 8](page-8.md)
* [Page 7](page-7.md)
* [Page 6](page-6.md)
* [Page 5](page-5.md)
* [Page 4](page-4.md)
* [Page 3](page-3.md)
* [Page 2](page-2.md)
* [Page 1](page-1.md)
* [Page](page.md)
