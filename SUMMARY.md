# Table of contents

* [ğŸŸ¢ Roadmap](README.md)
* [ğŸŸ¢ Practical use cases of NLP](practical-use-cases-of-nlp.md)
* [ğŸŸ¢ Approaches to NLP](approaches-to-nlp.md)
* [ğŸŸ¢ End to End NLP Pipeline](end-to-end-nlp-pipeline.md)
* [ğŸŸ¢ Syntax, Semantic and Pragmatics](syntax-semantic-and-pragmatics.md)
* [âœˆï¸ Pre-processing steps](pre-processing-steps.md)
* [ğŸ”´ Text cleaning](text-cleaning.md)
* [ğŸŸ¢ Tokenization and Basic Techniques](tokenization-and-basic-techniques.md)
* [ğŸŸ¢ Tokenization Practicals](tokenization-practicals.md)
* [ğŸŸ¢ Stemming using NLTK](stemming-using-nltk.md)
* [âœˆï¸ Stemming](stemming.md)
* [ğŸŸ¢ Lemmatization using NLTK](lemmatization-using-nltk.md)
* [âœˆï¸ Lemmatization](lemmatization.md)
* [ğŸ ---------- 10 ----------](10.md)
* [ğŸŸ¢ Stopwords](stopwords.md)
* [ğŸŸ¢ POS using NLTK](pos-using-nltk.md)
* [ğŸŸ¢ Named Entity Recognition](named-entity-recognition.md)
* [âœˆï¸ What's Next](whats-next.md)
* [ğŸŸ¢ Encoding vs Embedding](encoding-vs-embedding.md)
* [âœˆï¸ Encoding and Embedding](encoding-and-embedding.md)
* [âœˆï¸ Vector](vector.md)
* [ğŸŸ¢ One Hot Encoding](one-hot-encoding.md)
* [âœˆï¸ One Hot Encoding - 2](one-hot-encoding-2.md)
* [ğŸŸ¢ OHE Advantages and Disadvantages](ohe-advantages-and-disadvantages.md)
* [ğŸŸ¢ Bag of words intuition](bag-of-words-intuition.md)
* [ğŸŸ¢ BOW Advantages and Disadvantages](bow-advantages-and-disadvantages.md)
* [ğŸŸ¢ BOW implementation using NLTK](bow-implementation-using-nltk.md)
* [ğŸŸ¢ N Grams](n-grams.md)
* [ğŸ ----------  20 ----------](20.md)
* [âœˆï¸ N Grams - 2](n-grams-2.md)
* [ğŸŸ¢ N Gram BOW implementation using NLTK](n-gram-bow-implementation-using-nltk.md)
* [TF IDF Intuition](tf-idf-intuition.md)
* [ğŸŸ¢ TF-IDF](tf-idf.md)
* [ğŸŸ¢ TF IDF Advantages and Disadvantages](tf-idf-advantages-and-disadvantages.md)
* [ğŸŸ¢ TF IDF implementation](tf-idf-implementation.md)
* [ğŸŸ¢ BM25](bm25.md)
* [âœˆï¸ BM25 - Calculations](bm25-calculations.md)
* [ğŸŸ¢ Word Embeddings](word-embeddings.md)
* [ğŸŸ¢ Word2Vec intuition](word2vec-intuition.md)
* [ğŸŸ¢ Word2Vec Cbow Intuition](word2vec-cbow-intuition.md)
* [ğŸŸ¢ Skipgram Indepth Intuition](skipgram-indepth-intuition.md)
* [ğŸŸ¢ Advantages of Word2Vec](advantages-of-word2vec.md)
* [ğŸ ---------- 30  ----------](30.md)
* [ğŸŸ¢ Average Word2Vec intuition](average-word2vec-intuition.md)
* [âœˆï¸ Word2Vec - Training](word2vec-training.md)
* [ğŸŸ¢ Word2Vec - Custom training](word2vec-custom-training.md)
* [ğŸŸ¢ Word2Vec - Pre Trained](word2vec-pre-trained.md)
* [ğŸŸ¢ Spam Ham project using BOW](spam-ham-project-using-bow.md)
* [ğŸŸ¢ Spam Ham using Tf-Idf](spam-ham-using-tf-idf.md)
* [ğŸŸ¢ Best practices for solving ML problems](best-practices-for-solving-ml-problems.md)
* [âœˆï¸ Word2Vec - For your own data](word2vec-for-your-own-data.md)
* [ğŸŸ¢ Text Classification with Word2Vec and AvgWord2Vec](text-classification-with-word2vec-and-avgword2vec.md)
* [ğŸŸ¢ Kindle review sentiment analysis](kindle-review-sentiment-analysis.md)
* [ğŸŸ¢ Label Encoder](label-encoder.md)
* [âœˆï¸ Letter Embedding](letter-embedding.md)
* [âœˆï¸ Letter Embedding- Code](letter-embedding-code.md)
* [âœˆï¸ Code - Text Generation](code-text-generation.md)
* [ğŸŸ¢ Introduction to NLP in DL](introduction-to-nlp-in-dl.md)
* [ğŸ ---------- 40 ----------](40.md)
* [ğŸŸ¢ ANN vs RNN](ann-vs-rnn.md)
* [ğŸŸ¢ RNN Overview](rnn-overview.md)
* [ğŸŸ¢ Sequential Memory](sequential-memory.md)
* [ğŸŸ¢ Short term memory problem](short-term-memory-problem.md)
* [ğŸ”´ Data passing to RNN](data-passing-to-rnn.md)
* [ğŸŸ¢ Types of Configuration](types-of-configuration.md)
* [ğŸŸ¢ RNN forward propagation with time](rnn-forward-propagation-with-time.md)
* [ğŸŸ¢ Trainable Parameters](trainable-parameters.md)
* [ğŸŸ¢ RNN forward propagation with time](rnn-forward-propagation-with-time-1.md)
* [---------- 50 ----------](50.md)
* [ğŸ”´ Simple RNN Backward Propagation](simple-rnn-backward-propagation.md)
* [ğŸŸ¢ Derivative of Sigmoid](derivative-of-sigmoid.md)
* [ğŸ”´ Problems with RNN](problems-with-rnn.md)
* [ğŸŸ¢ Activation Function vs Vanishing Gradient](activation-function-vs-vanishing-gradient.md)
* [âœˆï¸ Equations](equations.md)
* [ğŸŸ¢ Code - Classifier](code-classifier.md)
* [ğŸŸ¢ Evaluation Metrics, Loss Function](evaluation-metrics-loss-function.md)
* [â„¹ï¸ End to End Deep Learning Project with RNN](end-to-end-deep-learning-project-with-rnn.md)
* [ğŸŸ¢ Problem Statement](problem-statement.md)
* [âœˆï¸ Getting started with embedding layers](getting-started-with-embedding-layers.md)
* [ğŸŸ¢ Implementing Word Embedding with Keras Tensorflow](implementing-word-embedding-with-keras-tensorflow.md)
* [ğŸŸ¢ Loading and Understanding Dataset and Feature Engineering](loading-and-understanding-dataset-and-feature-engineering.md)
* [ğŸŸ¢ Training simple RNN with embedding layers](training-simple-rnn-with-embedding-layers.md)
* [ğŸŸ¢ Prediction from trained Simple RNN](prediction-from-trained-simple-rnn.md)
* [---------- 60 ----------](60.md)
* [âœˆï¸ End to End Streamlit Web App Integrated with RNN and Deployment](end-to-end-streamlit-web-app-integrated-with-rnn-and-deployment.md)
* [ğŸŸ¢ Simple RNN with Pretrained embeddings](simple-rnn-with-pretrained-embeddings.md)
* [ğŸŸ¢ Why LSTM RNN](why-lstm-rnn.md)
* [ğŸŸ¢ Basic Representation of RNN and LSTM RNN](basic-representation-of-rnn-and-lstm-rnn.md)
* [ğŸŸ¢ LSTM Calculations](lstm-calculations.md)
* [ğŸŸ¢ LSTM RNN Architecture](lstm-rnn-architecture.md)
* [ğŸŸ¢ Forget Gate](forget-gate.md)
* [ğŸŸ¢ Input Gate and Candidate memory](input-gate-and-candidate-memory.md)
* [âœˆï¸ Combination of Forget and Input](combination-of-forget-and-input.md)
* [ğŸŸ¢ Output Gate](output-gate.md)
* [ğŸŸ¢ Variants of LSTM RNN - Peephole](variants-of-lstm-rnn-peephole.md)
* [ğŸŸ¢ Behavior of Network over time](behavior-of-network-over-time.md)
* [---------- 70 ----------](70.md)
* [âœˆï¸ Variant of LSTM](variant-of-lstm.md)
* [âœˆï¸ Stacking](stacking.md)
* [â„¹ï¸ LSTM and GRU End to End Project - Next Word Prediction](lstm-and-gru-end-to-end-project-next-word-prediction.md)
* [ğŸŸ¢ Data Collection and Pre Processing](data-collection-and-pre-processing.md)
* [ğŸŸ¢ Training](training.md)
* [ğŸŸ¢ Prediction](prediction.md)
* [âœˆï¸ Streamlit WebApp](streamlit-webapp.md)
* [âœˆï¸ GRU Main Simplifications](gru-main-simplifications.md)
* [ğŸŸ¢ GRU RNN](gru-rnn.md)
* [âœˆï¸ GRU Equations](gru-equations.md)
* [âœˆï¸ Types of RNN](types-of-rnn.md)
* [ğŸŸ¢ Bi RNN - Overview](bi-rnn-overview.md)
* [âœˆï¸ Bidirectional RNN](bidirectional-rnn.md)
* [âœˆï¸ Code - Sentiment Analysis](code-sentiment-analysis.md)
* [â„¹ï¸ Encoder Decoder](encoder-decoder.md)
* [Neural Machine Translation](neural-machine-translation.md)
* [ğŸŸ¢ Sequence Length](sequence-length.md)
* [Softmax](softmax.md)
* [Prediction](prediction-1.md)
* [Embedding Layer](embedding-layer.md)
* [---------- 80 ----------](80.md)
* [Intuiton](intuiton.md)
* [Why LSTM/GRU cannot be used for Language Translation](why-lstm-gru-cannot-be-used-for-language-translation.md)
* [Problems](problems.md)
* [Attention - Introduction](attention-introduction.md)
* [Key Points](key-points.md)
* [Self Attention](self-attention.md)
* [---------- 110 ----------](110.md)
* [Attention is all you need - Research Paper](attention-is-all-you-need-research-paper.md)
* [Attention is all you need](attention-is-all-you-need.md)
* [Attention Mechanism - Seq2Seq](attention-mechanism-seq2seq.md)
* [Attention Mechanism - Blog](attention-mechanism-blog.md)
* [ğŸŸ¢ What and Why to use Transformers](what-and-why-to-use-transformers.md)
* [ğŸŸ¢ Understanding the basic architecture of Encoder](understanding-the-basic-architecture-of-encoder.md)
* [ğŸŸ¢ Self attention layer working](self-attention-layer-working.md)
* [ğŸŸ¢ Multi head attention](multi-head-attention.md)
* [ğŸŸ¢ Feed Forward NN with Multi Head Attention](feed-forward-nn-with-multi-head-attention.md)
* [ğŸŸ¢ Positional Encoding](positional-encoding.md)
* [Layer Normalization](layer-normalization.md)
* [Batch Normalization vs Layer Normalization](batch-normalization-vs-layer-normalization.md)
* [Layer Normalization Example](layer-normalization-example.md)
* [Complete Encoder Transformer Architecture](complete-encoder-transformer-architecture.md)
* [Residual](residual.md)
* [Feed Forward NN](feed-forward-nn.md)
* [Decoder in Transformer](decoder-in-transformer.md)
* [Decoder - Masked Multi Head Attention](decoder-masked-multi-head-attention.md)
* [Masked Application](masked-application.md)
* [Types of Masking](types-of-masking.md)
* [Padding Mask](padding-mask.md)
* [Look Ahead Masking](look-ahead-masking.md)
* [---------- 120 ----------](120.md)
* [Dataloader and Dataset](dataloader-and-dataset.md)
* [Transformer Code](transformer-code.md)
* [Research Paper - Transformer](research-paper-transformer.md)
* [Research Paper - Transformer and ULMFit](research-paper-transformer-and-ulmfit.md)
* [ğŸ”´ Sentence Transformer](sentence-transformer.md)
* [ğŸŸ¢ Sentence embedding using sentence transformer](sentence-embedding-using-sentence-transformer.md)
* [ğŸŸ¢ Sentence embedding using Huggingface](sentence-embedding-using-huggingface.md)
* [Sentence embedding using transformers](sentence-embedding-using-transformers.md)
* [INTRODUCTION TO TRANSFORMERS FOR NLP](introduction-to-transformers-for-nlp.md)
* [1.1 A Brief History of NLP](1.1-a-brief-history-of-nlp.md)
* [1.2 Pay Attention with Attention](1.2-pay-attention-with-attention.md)
* [1.3 Encoder and Decoder Architecture](1.3-encoder-and-decoder-architecture.md)
* [1.4 How Language Model Looks at text](1.4-how-language-model-looks-at-text.md)
* [2.1 Introduction to Transformers](2.1-introduction-to-transformers.md)
* [---------- 130 ----------](130.md)
* [2.2 Scaled Dot Product Attention](2.2-scaled-dot-product-attention.md)
* [2.3 Multi Headed Attention](2.3-multi-headed-attention.md)
* [3.1 Introduction to Transfer Learning](3.1-introduction-to-transfer-learning.md)
* [3.2 Introduction to PyTorch](3.2-introduction-to-pytorch.md)
* [3.3 Fine tuning Transformers with PyTorch](3.3-fine-tuning-transformers-with-pytorch.md)
* [4.1 Introduction to BERT](4.1-introduction-to-bert.md)
* [4.2 Wordpiece Tokenization](4.2-wordpiece-tokenization.md)
* [4.3 The many embeddings of BERT](4.3-the-many-embeddings-of-bert.md)
* [5.1 The Masked Language Modelling Task](5.1-the-masked-language-modelling-task.md)
* [5.2 Next Sentence Prediction Task](5.2-next-sentence-prediction-task.md)
* [---------- 140 ----------](140.md)
* [5.3 Fine Tuning BERT to solve NLP Problems](5.3-fine-tuning-bert-to-solve-nlp-problems.md)
* [6.1 Flavors of BERT](6.1-flavors-of-bert.md)
* [6.2 BERT for Sequence Classification](6.2-bert-for-sequence-classification.md)
* [6.3 BERT for Token Classification](6.3-bert-for-token-classification.md)
* [6.4 BERT for QA](6.4-bert-for-qa.md)
