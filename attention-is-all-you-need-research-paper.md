---
hidden: true
---

# ✈️ Attention is all you need - Research Paper

* [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)

**Abstract:**

* Relies on attention mechanism
* Handle sequence in parallel, not step by step like RNN
* Faster to train

**Introduction:**

* **Goal:** Sequence transduction – map input sequence → output sequence (e.g., translation).
* **Previous models:**
  * **RNNs:** process sequences step by step; slow, hard to parallelize; struggle with long-range dependencies.
  * **CNNs:** capture multiple positions simultaneously; need many layers for long-range dependencies.
*



























