# Encoding vs Embedding



| Feature                  | **Encoding**                                  | **Embedding**                              |
| ------------------------ | --------------------------------------------- | ------------------------------------------ |
| ğŸ”¤ **Definition**        | Rule-based representation of words as numbers | Learned representation of words as vectors |
| ğŸ”¢ **Vector Type**       | Sparse (mostly 0s)                            | Dense (real numbers)                       |
| ğŸ§  **Captures Meaning?** | âŒ No                                          | âœ… Yes (semantic similarity)                |
| ğŸ§® **Dimensionality**    | High (equal to vocabulary size)               | Low (e.g., 100â€“768 dimensions)             |
| ğŸ§¾ **Representation**    | Fixed and manual                              | Learned from data                          |
| ğŸ” **Context Aware?**    | âŒ No (same vector always)                     | âœ… (contextual if using models like BERT)   |
| ğŸ“š **Examples**          | One-Hot, Bag of Words, TF-IDF                 | Word2Vec, GloVe, FastText, BERT            |
| ğŸ” **Similarity Info**   | Not captured                                  | Captured (similar words are close)         |
| âš™ï¸ **Use Case**          | Simple ML models                              | NLP models, deep learning, semantic tasks  |
