# Feed Forward NN

* To solve non linearity (By using activation function)
* Processing each position independently
  * Self attention captures relationship between tokens
  * Processes each token representation independently
  * This helps in transforming these representation further and allows the model to learn richer representation
* Adds depth to the model
  * More the depth, more learning will be there
