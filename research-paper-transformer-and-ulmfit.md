# Research Paper - Transformer and ULMFit

* Transformer has 2 parts - Encoder and Decoder
* As per ULMFit paper - it mentioned how we can use transfer learning in NLP domain
* The problem was they were using LSTM in architecture
* Later on they had to replace it with transformer architecture
* Both Transformers and ULMFit were published in 2017



**ULMFit:**

* Introduced concept of unsupervised pretraining
* Introduced concept of supervised finetuning
* All the models are using the same concept



RLHF - Introduced by OpenAI



